{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.callbacks import SaveModelCallback, EarlyStoppingCallback\n",
    "from fastprogress import progress_bar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from  torch import nn\n",
    "\n",
    "from henrys_model_utils import MSELossFlat, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '../data/resized_images'\n",
    "labels_path = '../data/resized_labels.csv'\n",
    "target_size = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(labels_path, index_col='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_func(x):\n",
    "    filename = str(x).split('/')[-1]\n",
    "    coord_list = []\n",
    "    coords = list(y.loc[filename])\n",
    "    for i in range(len(coords)//2):\n",
    "        coord_list.append([coords[i*2+1],coords[i*2]])\n",
    "    return tensor(coord_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = get_transforms(\n",
    "    do_flip=False,\n",
    "    max_rotate=45, \n",
    "    max_zoom=1.5, \n",
    "    max_lighting=0.5, \n",
    "    max_warp=0.4, \n",
    "    p_affine=1., \n",
    "    p_lighting=1.\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (\n",
    "    PointsItemList\n",
    "        .from_folder(images_path)\n",
    "        .split_by_rand_pct(0.15)\n",
    "        .label_from_func(get_y_func)\n",
    "        .transform(\n",
    "            transforms,\n",
    "            size=320,\n",
    "            tfm_y=True, \n",
    "            remove_out=False, \n",
    "            padding_mode='reflection', \n",
    "            resize_method=ResizeMethod.PAD\n",
    "        )\n",
    "        .databunch()\n",
    "        .normalize(imagenet_stats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.show_batch(3, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Reshape(nn.Module):\n",
    "#     def __init__(self, *args):\n",
    "#         super(Reshape, self).__init__()\n",
    "#         self.shape = args\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x.view(self.shape)\n",
    "    \n",
    "# class MSELossFlat(nn.MSELoss):\n",
    "#     def forward(self, input, target):\n",
    "#         return super().forward(input.view(-1), target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_reg = nn.Sequential(\n",
    "    Flatten(), \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(51200, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 22),\n",
    "    Reshape(-1,11,2),\n",
    "    nn.Tanh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_flat = MSELossFlat()\n",
    "\n",
    "learn = cnn_learner(\n",
    "    data, \n",
    "    models.resnet34, \n",
    "    loss_func=mse_loss_flat,\n",
    "    custom_head=head_reg\n",
    ").to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.path = Path('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.freeze_to(-1)\n",
    "# learn.fit_one_cycle(\n",
    "#     10, \n",
    "#     slice(1e-2), \n",
    "#     callbacks=[\n",
    "#         SaveModelCallback(learn, every='improvement', monitor='valid_loss', name='resnet34_frozen_to_-1'),\n",
    "#         EarlyStoppingCallback(learn, monitor='valid_loss', min_delta=0.01, patience=10)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load('fastai_resnet34')\n",
    "learn = learn.to_fp32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.freeze_to(-2)\n",
    "# learn.fit_one_cycle(\n",
    "#     10, \n",
    "#     slice(1e-3), \n",
    "#     callbacks=[\n",
    "#         SaveModelCallback(learn, every='improvement', monitor='valid_loss', name='resnet34_frozen_to_-2'),\n",
    "#         EarlyStoppingCallback(learn, monitor='valid_loss', min_delta=0.01, patience=10)\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.show_results(rows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to AWS Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pet_regression.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pet_regression.py\n",
    "from bentoml import BentoService, api, env, artifacts \n",
    "from bentoml.artifact import FastaiModelArtifact\n",
    "from bentoml.handlers import FastaiImageHandler\n",
    "\n",
    "@env(pip_dependencies=['fastai', 'henrys_model_utils'])\n",
    "@artifacts([FastaiModelArtifact('pet_regressor')])\n",
    "class PetRegression(BentoService):\n",
    "    \n",
    "    @api(FastaiImageHandler)\n",
    "    def predict(self, image):\n",
    "        result = self.artifacts.pet_regressor.predict(image)\n",
    "        return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-11-12 22:56:44,338] INFO - Successfully saved Bento 'PetRegression:20191112225617_DA68CF' to path: /Users/HenryDashwood/bentoml/repository/PetRegression/20191112225617_DA68CF\n"
     ]
    }
   ],
   "source": [
    "# 1) import the custom BentoService defined above\n",
    "from pet_regression import PetRegression\n",
    "\n",
    "# 2) `pack` it with required artifacts\n",
    "service = PetRegression.pack(pet_regressor=learn)\n",
    "\n",
    "# 3) save your BentoSerivce\n",
    "saved_path = service.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-11-12 22:56:54,558] WARNING - Module `pet_regression` already loaded, using existing imported module.\n",
      "(ImagePoints (320, 320), tensor([[-0.5638, -0.4552],\n",
      "        [-0.8983, -0.4173],\n",
      "        [-0.8985, -0.0678],\n",
      "        [-0.9187,  0.2398],\n",
      "        [-0.6159,  0.3104],\n",
      "        [-0.2388, -0.0278],\n",
      "        [-0.6171, -0.2045],\n",
      "        [-0.6355,  0.1022],\n",
      "        [-0.3086, -0.1297],\n",
      "        [-0.3481, -0.0332],\n",
      "        [-0.3190,  0.0545]]), tensor([[-0.5638, -0.4552],\n",
      "        [-0.8983, -0.4173],\n",
      "        [-0.8985, -0.0678],\n",
      "        [-0.9187,  0.2398],\n",
      "        [-0.6159,  0.3104],\n",
      "        [-0.2388, -0.0278],\n",
      "        [-0.6171, -0.2045],\n",
      "        [-0.6355,  0.1022],\n",
      "        [-0.3086, -0.1297],\n",
      "        [-0.3481, -0.0332],\n",
      "        [-0.3190,  0.0545]]))\n"
     ]
    }
   ],
   "source": [
    "from bentoml import load\n",
    "\n",
    "service = load(saved_path)\n",
    "\n",
    "print(service.predict(data.get(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/HenryDashwood/bentoml/repository/PetRegression/20191112225617_DA68CF\n",
      "Building wheels for collected packages: PetRegression\n",
      "  Building wheel for PetRegression (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PetRegression: filename=PetRegression-20191112225617_DA68CF-cp37-none-any.whl size=81256830 sha256=fa41be537f287ac158f2fc304c48c6ce0fb12bc9ea9e6a0e70aef36629877677\n",
      "  Stored in directory: /private/var/folders/yl/c5z1v7897q3g9ywlj63pm0840000gn/T/pip-ephem-wheel-cache-i6v3y0yn/wheels/ce/e9/16/db29e8f3810731edafa65c11b94103c0cdf78f14ec0df295e3\n",
      "Successfully built PetRegression\n",
      "Installing collected packages: PetRegression\n",
      "Successfully installed PetRegression-20191112225617-DA68CF\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade {saved_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ImagePoints (320, 320), tensor([[-0.3867,  0.0846],\n",
      "        [-0.6847,  0.0874],\n",
      "        [-0.5517,  0.2640],\n",
      "        [-0.6337,  0.4377],\n",
      "        [-0.3433,  0.4129],\n",
      "        [-0.1768,  0.2797],\n",
      "        [-0.3842,  0.1987],\n",
      "        [-0.3603,  0.3473],\n",
      "        [-0.2144,  0.2346],\n",
      "        [-0.2251,  0.2833],\n",
      "        [-0.2118,  0.3196]]), tensor([[-0.3867,  0.0846],\n",
      "        [-0.6847,  0.0874],\n",
      "        [-0.5517,  0.2640],\n",
      "        [-0.6337,  0.4377],\n",
      "        [-0.3433,  0.4129],\n",
      "        [-0.1768,  0.2797],\n",
      "        [-0.3842,  0.1987],\n",
      "        [-0.3603,  0.3473],\n",
      "        [-0.2144,  0.2346],\n",
      "        [-0.2251,  0.2833],\n",
      "        [-0.2118,  0.3196]]))\n"
     ]
    }
   ],
   "source": [
    "!PetRegression predict --input=../data/resized_images/Abyssinian_1.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bentoml serve {saved_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetRegression:20191112225617_DA68CF\n"
     ]
    }
   ],
   "source": [
    "bento_tag = '{name}:{version}'.format(name=service.name, version=service.version)\n",
    "print(bento_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-11-13 07:43:25,306] DEBUG - Using BentoML with local Yatai server\n",
      "[2019-11-13 07:43:25,942] DEBUG - Upgrading tables to the latest revision\n",
      "\u001b[31mFailed to delete deployment facial-features-detector. code: NOT_FOUND, message: Deployment \"facial-features-detector\" in namespace \"default\" not found\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!bentoml --verbose deployments delete facial-features-detector --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-11-12 23:08:24,143] DEBUG - Using BentoML with local Yatai server\n",
      "[2019-11-12 23:08:24,391] DEBUG - Upgrading tables to the latest revision\n",
      "[2019-11-12 23:08:24,793] DEBUG - Created temporary directory: /private/var/folders/yl/c5z1v7897q3g9ywlj63pm0840000gn/T/bentoml-temp-eqddothq\n",
      "[2019-11-12 23:09:53,867] DEBUG - sls cmd output: Serverless: Generating boilerplate...\n",
      " _______                             __\n",
      "|   _   .-----.----.--.--.-----.----|  .-----.-----.-----.\n",
      "|   |___|  -__|   _|  |  |  -__|   _|  |  -__|__ --|__ --|\n",
      "|____   |_____|__|  \\___/|_____|__| |__|_____|_____|_____|\n",
      "|   |   |             The Serverless Application Framework\n",
      "|       |                           serverless.com, v1.53.0\n",
      " -------'\n",
      "\n",
      "Serverless: Successfully generated boilerplate for template: \"aws-python3\"\n",
      "\n",
      "[2019-11-12 23:09:57,367] INFO - Installing additional packages: serverless-python-requirements\n",
      "[2019-11-12 23:10:15,064] DEBUG - sls cmd output: Serverless: Creating an empty package.json file in your service directory\n",
      "Serverless: Installing plugin \"serverless-python-requirements@latest\" (this might take a few seconds...)\n",
      "Serverless: Successfully installed \"serverless-python-requirements@latest\"\n",
      "\n",
      "[2019-11-12 23:10:15,064] INFO - Deploying to AWS Lambda\n",
      "[2019-11-13 00:36:16,066] DEBUG - sls cmd output: Serverless: Adding Python requirements helper...\n",
      "Serverless: Generated requirements from /private/var/folders/yl/c5z1v7897q3g9ywlj63pm0840000gn/T/bentoml-temp-eqddothq/requirements.txt in /private/var/folders/yl/c5z1v7897q3g9ywlj63pm0840000gn/T/bentoml-temp-eqddothq/.serverless/requirements.txt...\n",
      "Serverless: Installing requirements from /Users/HenryDashwood/Library/Caches/serverless-python-requirements/a6d4fcb919a0c2535a7c9c9cc66f504357874a04c077d28d07d81109465c3604_slspyc/requirements.txt ...\n",
      "Serverless: Docker Image: lambci/lambda:build-python3.7\n",
      "Serverless: Using download cache directory /Users/HenryDashwood/Library/Caches/serverless-python-requirements/downloadCacheslspyc\n",
      "Serverless: Running docker run --rm -v /Users/HenryDashwood/Library/Caches/serverless-python-requirements/a6d4fcb919a0c2535a7c9c9cc66f504357874a04c077d28d07d81109465c3604_slspyc\\:/var/task\\:z -v /Users/HenryDashwood/Library/Caches/serverless-python-requirements/downloadCacheslspyc\\:/var/useDownloadCache\\:z -u 0 -v /private/var/folders/yl/c5z1v7897q3g9ywlj63pm0840000gn/T/bentoml-temp-eqddothq/bundled_pip_dependencies\\:/var/task/bundled_pip_dependencies\\:z lambci/lambda\\:build-python3.7 /bin/sh -c 'python3.7 -m pip install -t /var/task/ -r /var/task/requirements.txt --cache-dir /var/useDownloadCache && find /var/task -name \\\\*.so -exec strip \\\\{\\\\} \\\\;'...\n",
      "Serverless: Zipping required Python packages...\n",
      "Serverless: Packaging service...\n",
      "Serverless: Excluding development dependencies...\n",
      "Serverless: Removing Python requirements helper...\n",
      "Serverless: Injecting required Python packages to package...\n",
      "Serverless: Creating Stack...\n",
      "Serverless: Checking Stack create progress...\n",
      ".....\n",
      "Serverless: Stack create finished...\n",
      "Serverless: Uploading CloudFormation file to S3...\n",
      "Serverless: Uploading artifacts...\n",
      "Serverless: Uploading service facial-features-detector.zip file to S3 (862.69 MB)...\n",
      "Serverless: Validating template...\n",
      "Serverless: Updating Stack...\n",
      "Serverless: Checking Stack update progress...\n",
      "................\n",
      "Serverless: Operation failed!\n",
      "Serverless: View the full error output: https://us-east-1.console.aws.amazon.com/cloudformation/home?region=us-east-1#/stack/detail?stackId=arn%3Aaws%3Acloudformation%3Aus-east-1%3A235858150097%3Astack%2Ffacial-features-detector-default%2Fb8a403f0-05a9-11ea-b46c-122e54527a47\n",
      " \n",
      "  Serverless Error ---------------------------------------\n",
      " \n",
      "  An error occurred: PredictLambdaFunction - Unzipped size must be smaller than 262144000 bytes (Service: AWSLambdaInternal; Status Code: 400; Error Code: InvalidParameterValueException; Request ID: 0cea3bbb-61de-45a6-b66f-dcd6d4fb6ba6).\n",
      " \n",
      "  Get Support --------------------------------------------\n",
      "     Docs:          docs.serverless.com\n",
      "     Bugs:          github.com/serverless/serverless/issues\n",
      "     Issues:        forum.serverless.com\n",
      " \n",
      "  Your Environment Information ---------------------------\n",
      "     Operating System:          darwin\n",
      "     Node Version:              10.16.3\n",
      "     Framework Version:         1.53.0\n",
      "     Plugin Version:            3.2.3\n",
      "     SDK Version:               2.2.1\n",
      "     Components Core Version:   1.1.2\n",
      "     Components CLI Version:    1.4.0\n",
      " \n",
      "\n",
      "\u001b[31mFailed to create deployment facial-features-detector. INTERNAL:   An error occurred: PredictLambdaFunction - Unzipped size must be smaller than 262144000 bytes (Service: AWSLambdaInternal; Status Code: 400; Error Code: InvalidParameterValueException; Request ID: 0cea3bbb-61de-45a6-b66f-dcd6d4fb6ba6).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!bentoml --verbose deployment create facial-features-detector --bento {bento_tag} --platform aws-lambda --region us-east-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bentoml deployment list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniconda3-latest",
   "language": "python",
   "name": "miniconda3-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
